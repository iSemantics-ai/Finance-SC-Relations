{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1508b67b-9ef8-4754-a939-b878a1de1945",
   "metadata": {},
   "source": [
    "# agg_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4148dd9-3d2d-44e1-b5a2-83db8dc74fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections.abc import Iterable as iterable\n",
    "from typing import List, Text,Tuple, Iterable, Dict\n",
    "from itertools import chain\n",
    "import random\n",
    "from collections.abc import Iterable as iterable\n",
    "\n",
    "# method to add Label based on the relations columns\n",
    "def sc_label_from_relations(relation_tuples, main_relations):\n",
    "    if not relation_tuples:\n",
    "        return 0\n",
    "    if len(relation_tuples) == 0:\n",
    "        return 0\n",
    "    for relation_tuple in relation_tuples:\n",
    "        if len(relation_tuple) != 3:\n",
    "            continue\n",
    "        elif  relation_tuple[1] in main_relations:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def check_relation_tuples(relations: List[Iterable]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the relations list is in the correct format.\n",
    "    \"\"\"\n",
    "    if not all(isinstance(relation, iterable) and len(relation) == 3 for relation in relations):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def return_possible_pairs(ids_set:List):\n",
    "    return list(zip(\n",
    "            list(chain(*[[ids_set[x]]*(len(ids_set)-1-x) \\\n",
    "                 for x in range(len(ids_set))])),\n",
    "            list(chain(*[[ids_set[i] for i in range(x+1, len(ids_set))]\\\n",
    "                 for x in range(len(ids_set))]))))\n",
    "\n",
    "\n",
    "def get_other_relations(ids2org):\n",
    "    \"\"\"\n",
    "    Returns a list of other relations between companies based on the dictionary of company groups passed as input.\n",
    "    The maximum number of other relations is determined by the max_others parameter.\n",
    "\n",
    "    @params\n",
    "    ids2org\n",
    "    org_groups (dict): A dictionary with company ids to map each group of ents.\n",
    "    max_others (int): The maximum number of other relations to return.\n",
    "\n",
    "    @returns\n",
    "    --------\n",
    "    list: A list of other relations between companies.\n",
    "    \"\"\"\n",
    "    # Sort company keys (IDs) in ascending order\n",
    "    comp_keys = sorted(ids2org.keys())\n",
    "\n",
    "    # Generate all possible pairs of IDs\n",
    "    other_ids = set(return_possible_pairs(comp_keys))\n",
    "\n",
    "    # Create 'other_relations' tuples for each pair of IDs and return\n",
    "    return [(ids2org[pair[0]][0] , 'other', ids2org[pair[1]][0]) for pair in other_ids]\n",
    "\n",
    "\n",
    "\n",
    "def eval_relations(relation, default=[]):\n",
    "    try:\n",
    "        re = eval(relation)\n",
    "    except:\n",
    "        re = default\n",
    "    return re\n",
    "\n",
    "def eval_relation_data(output, relations_map):\n",
    "    \"\"\"\n",
    "    Evaluate and process relation data in the output DataFrame.\n",
    "\n",
    "    @params:\n",
    "    - output: DataFrame containing relation data.\n",
    "    - relations_map: Dictionary mapping relations.\n",
    "\n",
    "    @returns:\n",
    "    - None (modifies 'output' DataFrame in place).\n",
    "    \"\"\"\n",
    "    # Resort sme_relations to unify the relations directions\n",
    "    if not isinstance(output['sme_relations'].iloc[0], list):\n",
    "        tqdm.pandas(desc=\"Eval sme_relations\")\n",
    "        output['sme_relations'] = output['sme_relations'].progress_apply(eval)\n",
    "\n",
    "    # Evaluate and process 'relations' column if it exists\n",
    "    if 'relations' in output.columns:\n",
    "        if not isinstance(output['relations'].iloc[0], list):\n",
    "            tqdm.pandas(desc=\"Eval relations\")\n",
    "            output['relations'] = output['relations'].progress_apply(eval_relations)\n",
    "    else:\n",
    "        # If 'relations' column doesn't exist, create it with None values\n",
    "        output['relations'] = None\n",
    "    \n",
    "    # Evaluate and process 'org_groups' column\n",
    "    if not isinstance(output['org_groups'].iloc[0], dict):\n",
    "        tqdm.pandas(desc=\"Eval org_groups\")\n",
    "        output['org_groups'] = output['org_groups'].progress_apply(eval_relations, default={})\n",
    "    \n",
    "    # Resort sme_relations based on the provided 'relations_map'\n",
    "    tqdm.pandas(desc=\"Resort sme relations\")\n",
    "    output['sme_relations'] = output['sme_relations'].progress_apply(lambda x: \\\n",
    "                              resort_relation((x[0], x[1], x[2]),\n",
    "                                            relations_map))\n",
    "\n",
    "def resort_relation(relation_tuple:Tuple, relations_map:Dict)->Tuple:\n",
    "    \"\"\"Resorts a tuple to match the order of the main relation.\"\"\"\n",
    "    c1, relation, c2 = relation_tuple\n",
    "    return [c1, relation, c2]\\\n",
    "            if not relations_map.get(relation)\\\n",
    "            else [c2, relations_map.get(relation), c1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2aee37-245b-481f-a229-3b6ac998cf35",
   "metadata": {},
   "source": [
    "# data_aggregator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c4f737-5707-46aa-9327-2e601c596fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Tuple, List, Text, Dict\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import random\n",
    "class DataAggregator():\n",
    "    '''Aggregating the annoated files from the LLMs from multiple files        \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 dataset_name,\n",
    "                 output_dir,\n",
    "                 entity_matcher,\n",
    "                 relation_direction={\"customer\":\"supplier\"},\n",
    "                 filer_names=['entity_1', 'firstEntity', 'filer' ],\n",
    "                 relations_key = 'relations',\n",
    "                 text_col= 'sentence',\n",
    "                 lm_name=\"en_core_web_trf\",\n",
    "                 lm_type='spacy',\n",
    "                 ):\n",
    "\n",
    "        # Define the basic information about the files which will be used\n",
    "        self._dataset_name = dataset_name\n",
    "        self.filer_names = filer_names\n",
    "        self.text_col= text_col\n",
    "        self.relations_key=relations_key\n",
    "        \n",
    "\n",
    "        self.relation_direction = relation_direction\n",
    "        self.main_relations = list(self.relation_direction.keys()) + list(self.relation_direction.values())\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        with open(dataset_name) as o:\n",
    "            self.data_files = yaml.safe_load(o)\n",
    "        \n",
    "        # Construct language model for entity extraction and matching\n",
    "        if lm_type=='spacy':\n",
    "            from src.language_model.spacy_loader import SpacyLoader\n",
    "            self.lm = SpacyLoader(lm_name,\n",
    "                                  entity_matcher= entity_matcher,\n",
    "                                  load_matcher=True)\n",
    "\n",
    "    def read_and_prepare_datafiles(self):\n",
    "        # read file and store each into it's coresponding key_value\n",
    "        for key in list(self.data_files.keys()):\n",
    "            print(key,'\\n--------------')\n",
    "            self.data_files[key]['data'] = pd.read_excel(self.data_files[key]['dir'])\n",
    "            self.data_files[key]['data'] = self.process_labeled_data(self.data_files[key]['data']).reset_index(drop=True)\n",
    "            self.data_files[key]['data']['idx'] = [\"{}_{}\".format(key, i)\\\n",
    "                                            for i in range(self.data_files[key]['data'].shape[0])]\n",
    "            self.data_files[key]['data']['Label'] = self.data_files[key]['data']['relations']\\\n",
    "                                                .apply(lambda x : sc_label_from_relations(x, self.main_relations))        \n",
    "\n",
    "    def process_labeled_data(self,\n",
    "                             data:pd.DataFrame): \n",
    "        \"\"\"\n",
    "        Process labeled data by creating 'org_groups' if not existent and deserializing data.\n",
    "\n",
    "        @params:\n",
    "        - data: DataFrame containing labeled data.\n",
    "        - text_col: Column containing text data.\n",
    "        - relation_direction: Dictionary mapping relations.\n",
    "\n",
    "        @returns:\n",
    "        - None (modifies 'data' DataFrame in place).\n",
    "        \"\"\"\n",
    "        # Create 'org_groups' if not existent\n",
    "        if not all([x in data.columns for x in ['spans', 'org_groups']]):\n",
    "            sents, spans, org_groups, aliases = self.lm.predictor(data[self.text_col])\n",
    "            data[self.text_col] = sents\n",
    "            data.loc[:, self.text_col] = sents\n",
    "            data.loc[:, \"spans\"] = spans\n",
    "            data.loc[:, \"org_groups\"] = org_groups\n",
    "        # Deserialize data and evaluate relations\n",
    "        eval_relation_data(data, self.relation_direction)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def create_re_dataset(self,\n",
    "                          data,\n",
    "                          threshold:float=0.9,\n",
    "                          max_others:int=3,\n",
    "                          basic_columns:list=[],\n",
    "                          only_filer=False,\n",
    "                          ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a relation extraction dataset.\n",
    "\n",
    "        @params\n",
    "        -------\n",
    "        - data: A pandas dataframe containing the following keys:\n",
    "            * sentence: A string representing a sentence containing relevant text.\n",
    "            * relations: A list of tuples representing the relationship between companies.\n",
    "\n",
    "        @returns\n",
    "        --------\n",
    "        - dataset: A pandas dataframe containing the following columns:\n",
    "        - idx: A unique identifier for the datapoint.\n",
    "        - sentence: A string representing the sentence containing relevant text.\n",
    "        - entity_2: A string representing the second entity in the relation.\n",
    "        - relation: A string representing the relation between the two entities.\n",
    "        - entity_1: A string representing the first entity in the relation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the `extract_relations_from_llm` function to each datapoint in the data dataframe\n",
    "        tqdm.pandas(desc=\"extract relations\")\n",
    "        results = data.progress_apply(lambda x: self.extract_relations_from_llm(datapoint=x,\n",
    "                                threshold=threshold,\n",
    "                                max_others=max_others,\n",
    "                                only_filer = only_filer), axis=1)\n",
    "\n",
    "        # Create new columns in the output dataframe to store the results of the `extract_relations_from_llm` function\n",
    "        data['llm_relations']  = results.apply(lambda x : x[0]).tolist()\n",
    "        data['other_relations']  = results.apply(lambda x : x[1]).tolist()\n",
    "        relation_columns = ['llm_relations', 'other_relations']\n",
    "        # Create a list of all possible pair-wise combinations of the values in 'ids2org', and randomly choose 5 of those combinations\n",
    "        columns = relation_columns + [self.text_col, self.relations_key] + basic_columns\n",
    "        re_dataset = []\n",
    "        for _, row in data[columns].iterrows():\n",
    "            row = row.to_dict()\n",
    "            for r_column in relation_columns:\n",
    "                # Iterate over relations and ingest row for each relation\n",
    "                for relation_tuple in row[r_column]:\n",
    "                    row['entity_2'] = relation_tuple[0]\n",
    "                    row['relation'] = relation_tuple[1]\n",
    "                    row['entity_1'] = relation_tuple[2]\n",
    "                    re_dataset.append(dict(row))\n",
    "\n",
    "        # Return a pandas dataframe containing the relation extraction dataset\n",
    "        dataset = pd.DataFrame(re_dataset)[[ self.text_col,\n",
    "                                            'entity_2',\n",
    "                                            'relation',\n",
    "                                            'entity_1'] + basic_columns]\n",
    "        return dataset\n",
    "\n",
    "        \n",
    "    def extract_relations_from_llm(self,\n",
    "                                   datapoint,\n",
    "                                   threshold:float=0.9,\n",
    "                                   only_filer = False,\n",
    "                                   max_others=3):\n",
    "        \"\"\"\n",
    "        Create a dataset for relation extraction training.\n",
    "        @params\n",
    "        -------\n",
    "        - datapoint: A dictionary containing the following keys:\n",
    "             * org_groups: A dictionary of company names associated with an integer identifier.\n",
    "             * relations: A list of tuples representing the relationship between companies.\n",
    "        - threshold: The similarity threshold for matching company names.\n",
    "\n",
    "        @returns\n",
    "        --------\n",
    "        - llms_relations: A list of tuples representing the relationships between companies that were successfully matched.\n",
    "        - other_relations: A list of tuples representing the relationships between companies that were not matched.\n",
    "\n",
    "        @raises\n",
    "        -------\n",
    "        - ValueError: If the relations list in the datapoint is invalid.\n",
    "        \"\"\"\n",
    "        r_others = True\n",
    "        # establish org_groups\n",
    "        group2id = datapoint['org_groups']\n",
    "        id2group = defaultdict(list)\n",
    "        for k,v in group2id.items():\n",
    "            id2group[v].append(k)\n",
    "\n",
    "        # define llms relations\n",
    "        relations = datapoint[self.relations_key]\n",
    "\n",
    "        # build index for org_groups\n",
    "        if len(group2id) > 0:\n",
    "            self.lm.entity_matcher.build_index(list(group2id.keys()))\n",
    "\n",
    "        # Assert the relations on the right format\n",
    "        if not check_relation_tuples(relations):\n",
    "            raise ValueError(\"Invlid relations list on the datapoint, must be List[Tuple[Text, Text, Text]]\")\n",
    "        # Collect all companies mentioned in the relations and create a dictionary with each unique company as a key\n",
    "        llms_companies = set()\n",
    "        if isinstance(relations, list):\n",
    "            llms_companies = list(set(chain(*[[x[0], x[2]] for x in relations])))\n",
    "\n",
    "        # match the llm_companies to assign id according to group2id\n",
    "        llms_co_matches = self.lm.entity_matcher.search(llms_companies, threshold=threshold, top_k=2)\\\n",
    "                          if len(llms_companies) > 0 else []\n",
    "        # Create map the merge org_groups with llm_companies\n",
    "        llms_ids = {}\n",
    "        for co_match, llm_company in zip(llms_co_matches, llms_companies):\n",
    "            # If match found\n",
    "            if len(co_match) > 0:\n",
    "                llms_ids[llm_company] = group2id[co_match[0][0]]\n",
    "\n",
    "            # check if llm_company valid and add it to \n",
    "            elif llm_company in datapoint[self.text_col]:\n",
    "                group2id[llm_company] = max(id2group.keys()) + 1 if len(id2group.keys()) > 0 else 1\n",
    "                id2group[group2id[llm_company]] = [llm_company]\n",
    "                llms_ids[llm_company] = group2id[llm_company]\n",
    "\n",
    "        # Create a dictionary mapping IDs to company names\n",
    "        llms_names = {k: id2group[v][0] for k, v in llms_ids.items()}\n",
    "\n",
    "        # get all possible paris from the llms_ids\n",
    "        availabel_relations = return_possible_pairs(sorted(set(group2id.values())))\n",
    "\n",
    "        # Define all the exist relations from LLM with pairs tuples\n",
    "        exist_relations = []\n",
    "        llms_relations = []\n",
    "        if isinstance(relations, list):\n",
    "            for relation in relations:\n",
    "                c1, c1_name = relation[0], llms_names.get(relation[0])\n",
    "                c2, c2_name = relation[2], llms_names.get(relation[2])\n",
    "                c1_id = llms_ids.get(c1)\n",
    "                c2_id = llms_ids.get(c2)\n",
    "                if None in [c1_id, c2_id]:\n",
    "                    continue\n",
    "                llms_relations.append((c1_name, relation[1], c2_name))\n",
    "                exist_relations.append(tuple(sorted([c1_id, c2_id])))\n",
    "        # Define all possible other relation pairs within the sentence\n",
    "        other_ids = list(set(availabel_relations) ^ set(exist_relations))\n",
    "        \n",
    "        # Create relations tuples for other_relations\n",
    "        other_relations = [(id2group[pair[0]][0],\n",
    "                            'other',\n",
    "                            id2group[pair[1]][0]) \\\n",
    "                           for pair in other_ids]\n",
    "        # If llm return nothign, get all possible relations as `other`\n",
    "        if len(llms_relations) == 0 and len(other_relations) == 0:\n",
    "            other_relations = get_other_relations(id2group)\n",
    "        # If only filer include only relations with filer\n",
    "        if only_filer:\n",
    "            # Find Filer name as mentioned on the sentence\n",
    "            filer_column = list(set(datapoint.keys()).intersection(self.filer_names))\n",
    "            filer_column = filer_column[0] if len(filer_column) > 0 else None\n",
    "            given_filer = datapoint[filer_column] if filer_column else None\n",
    "            if given_filer:\n",
    "                filer_name = group2id.get(given_filer)\n",
    "                if not filer_name and len(group2id) > 0:\n",
    "                    filer_scope = list(group2id.keys())\n",
    "                    filer_sim = self.lm.entity_matcher.similarity(given_filer,filer_scope)\n",
    "                    if filer_sim.max() > threshold:\n",
    "                        filer_name = filer_scope[filer_sim.argmax()]\n",
    "            if filer_name:\n",
    "                llms_relations = list(filter(None, [x if filer_name in [x[0], x[2]]\\\n",
    "                                                    else None  for x in llms_relations] ))\n",
    "                other_relations = list(filter(None, [x if filer_name in [x[0], x[2]] \\\n",
    "                                                     else None  for x in other_relations] ))\n",
    "        # Based on max_other return random sample\n",
    "        other_relations = random.sample(other_relations, min(len(other_relations), max_others))\n",
    "        return llms_relations, other_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c5a3775-23b0-4ded-b0de-2093a1e577f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer', 'supplier']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aggregator.main_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa22178-e85e-4bf4-927c-1b544d0334f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 08:51:42,049 — 🌌 spaCy — INFO — Language model used is en_core_web_trf\n",
      "2023-12-18 08:51:42,050 — 🌌 spaCy — INFO — spaCy Work On GPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "src_dir = Path.cwd().parent\n",
    "sys.path.append(str(src_dir))\n",
    "data_aggregator = DataAggregator(dataset_name= src_dir / 'data/config/llm_aligned_0_1_huge_complex.yaml',\n",
    "                                 output_dir= src_dir /'data/raw/aggregated_data.json',\n",
    "                                 entity_matcher= str(src_dir/\"artifacts/matcher_model/\")\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe12dd47-021e-4ae0-9785-76d9aa9350b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_0 \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval sme_relations: 100%|██████████| 877/877 [00:00<00:00, 112221.75it/s]\n",
      "Eval relations: 100%|██████████| 877/877 [00:00<00:00, 86503.88it/s]\n",
      "Resort sme relations: 100%|██████████| 877/877 [00:00<00:00, 608503.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_1 \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval sme_relations: 100%|██████████| 1767/1767 [00:00<00:00, 113383.85it/s]\n",
      "Eval relations: 100%|██████████| 1767/1767 [00:00<00:00, 68599.97it/s]\n",
      "Resort sme relations: 100%|██████████| 1767/1767 [00:00<00:00, 439112.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_other \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval sme_relations: 100%|██████████| 111/111 [00:00<00:00, 83931.45it/s]\n",
      "Eval relations: 100%|██████████| 111/111 [00:00<00:00, 234897.95it/s]\n",
      "Resort sme relations: 100%|██████████| 111/111 [00:00<00:00, 277288.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huge_1 \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval sme_relations: 100%|██████████| 688/688 [00:00<00:00, 113252.79it/s]\n",
      "Eval relations: 100%|██████████| 688/688 [00:00<00:00, 70461.52it/s]\n",
      "Resort sme relations: 100%|██████████| 688/688 [00:00<00:00, 524002.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huge_1_complex \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval sme_relations: 100%|██████████| 1550/1550 [00:00<00:00, 112284.69it/s]\n",
      "Eval relations: 100%|██████████| 1550/1550 [00:00<00:00, 76231.46it/s]\n",
      "Resort sme relations: 100%|██████████| 1550/1550 [00:00<00:00, 562677.10it/s]\n"
     ]
    }
   ],
   "source": [
    "data_aggregator.read_and_prepare_datafiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3045320d-d98f-410c-973e-5e899fe3d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_0 \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract relations: 100%|██████████| 877/877 [02:19<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_1 \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract relations: 100%|██████████| 1767/1767 [04:49<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_other \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract relations: 100%|██████████| 111/111 [00:17<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huge_1 \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract relations: 100%|██████████| 688/688 [01:52<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huge_1_complex \n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract relations: 100%|██████████| 1550/1550 [04:15<00:00,  6.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# read file and store each into it's coresponding key_value\n",
    "for key in list(data_aggregator.data_files.keys()):\n",
    "    print(key,'\\n--------------' )\n",
    "    dataset = data_aggregator.create_re_dataset(data=data_aggregator.data_files[key]['data'].reset_index(drop=True),\n",
    "                           threshold=0.9,\n",
    "                           max_others=2,\n",
    "                           basic_columns=['Label', 'concept_class', 'idx', 'org_groups', 'spans', 'sme_relations'],\n",
    "                           only_filer=True)\n",
    "    data_aggregator.data_files[key]['dataset'] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d07df3-8342-4a34-94d1-6b8c72474ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c0d9500-0ab3-477a-a1a4-0e6eff667d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset= pd.concat([data_aggregator.data_files[k]['dataset'] for k in data_aggregator.data_files.keys()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd8dc8ee-205c-4d63-8c4b-b428eda40cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e11c71-bfd1-4c12-b747-171e2d147d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concept_class_remapping = {'agreement': ['supply_purchase_agreement',\n",
    "               'services agreement',\n",
    "               'agreement_and_partnership'],\n",
    "'licensing_and_ip': ['royalties',\n",
    "                     'licensing_and_ip',\n",
    "                     'legal_and_regulatory'],\n",
    "'supply_chain': ['supply_chain', 'product_related'],\n",
    "'revenue': ['revenue', 'royalties'],\n",
    "'real_estate': ['real_estate'],\n",
    "'financial_statements': ['investment_related', 'financial_statements'],\n",
    "'other': ['unknown'] \n",
    "}\n",
    "\n",
    "# reverse the mapping\n",
    "reverse_concept_class_remapping = {}\n",
    "for k,v in concept_class_remapping.items():\n",
    "    for x in v:\n",
    "        reverse_concept_class_remapping[x] = k  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7db5a2a-a62e-4f16-8bd0-47fb0c003b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the concept class\n",
    "all_dataset[\"concept_class_remapped\"] = all_dataset[\"concept_class\"].apply(lambda x: reverse_concept_class_remapping.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9857a8b5-925d-4851-96d0-ee1592028516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't include other class\n",
    "all_dataset = all_dataset[all_dataset[\"concept_class_remapped\"] != \"other\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e0819-afe5-40cc-a3b0-cf6b7627352e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4bb43a8-eb12-4a44-b027-af021954fb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revenue                 3263\n",
       "supply_chain            1102\n",
       "agreement                875\n",
       "licensing_and_ip         601\n",
       "financial_statements     523\n",
       "real_estate              133\n",
       "Name: concept_class_remapped, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset[\"concept_class_remapped\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "888189d0-58f4-40ca-aca6-e232a82e5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.preprocess import word_search\n",
    "founded1 = all_dataset.apply(lambda x : word_search(x['entity_1'], x['sentence']), axis =1 )\n",
    "founded2 = all_dataset.apply(lambda x : word_search(x['entity_2'], x['sentence']), axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aae5c95a-a067-4244-9fa2-a4d32d37c810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(founded1.apply(len) == 0).sum()==0,(founded2.apply(len) == 0).sum()==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b06bdd-c66f-486e-90fb-d0d545522a33",
   "metadata": {},
   "source": [
    "# test_re_dataset_creation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "031b1e38-986c-45d7-b1c2-c45df191b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Text, Iterable\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Text\n",
    "import random\n",
    "from collections.abc import Iterable as iterable\n",
    "\n",
    "\n",
    "def test_check_relation_tuples():\n",
    "    assert check_relation_tuples([]) == True\n",
    "    assert check_relation_tuples([(1, 2, 3)]) == True\n",
    "    assert check_relation_tuples([(1, 2)]) == False\n",
    "    assert check_relation_tuples([(1, 2, 3), (4, 5, 6), (7, 8, 9)]) == True\n",
    "\n",
    "def test_return_possible_pairs():\n",
    "    assert return_possible_pairs([1, 2, 3]) == [(1, 2), (1, 3), (2, 3)]\n",
    "    assert return_possible_pairs([]) == []\n",
    "\n",
    "\n",
    "test_check_relation_tuples()\n",
    "test_return_possible_pairs()\n",
    "\n",
    "\n",
    "def test_point():\n",
    "    return {\n",
    " 'filer': 'ADVANCED MICRO DEVICES INC corp',\n",
    " 'sentence': 'In addition, five customers, including Sony and Microsoft, accounted for approximately 95% of the net revenue attributable to ADVANCED MICRO DEVICES Inc Enterprise, Embedded and Semi Custom segment',\n",
    " 'relations': [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft'],\n",
    "              ],\n",
    " 'org_groups': {'ADVANCED MICRO DEVICES Inc': 0, 'Microsoft': 1, 'Sony': 2}}\n",
    "\n",
    "\n",
    "############################### Test only_filer=true ###############################\n",
    "\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(test_point(),\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= True,\n",
    "                            max_others= 1) \n",
    "assert llms_relations == [('ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'),\n",
    "                          ('ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft')]\n",
    "assert other_relations == []\n",
    "\n",
    "############################### Test only_filer=False & max_other=1 ###############################\n",
    "\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(test_point(),\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= False,\n",
    "                            max_others= 1) \n",
    "assert llms_relations == [('ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'),\n",
    "                          ('ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft')]\n",
    "assert other_relations == [('Microsoft', 'other', 'Sony')]\n",
    "\n",
    "############################### Test Changing Names slightly ###############################\n",
    "datapoint = test_point()\n",
    "datapoint['relations'] = [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony Inc'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft inc'],\n",
    "              ]\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(datapoint,\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= False,\n",
    "                            max_others= 1) \n",
    "assert llms_relations == [('ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'),\n",
    "                          ('ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft')]\n",
    "assert other_relations == [('Microsoft', 'other', 'Sony' )]\n",
    "\n",
    "\n",
    "############################### Test When All Others ###############################\n",
    "datapoint = test_point()\n",
    "datapoint['relations'] = [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'other', 'Sony Inc'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'other', 'Microsoft inc'],\n",
    "              ]\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(datapoint,\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= False,\n",
    "                            max_others= 1) \n",
    "assert llms_relations ==[('ADVANCED MICRO DEVICES Inc', 'other', 'Sony'),\n",
    " ('ADVANCED MICRO DEVICES Inc', 'other', 'Microsoft')]\n",
    "assert other_relations == [('Microsoft', 'other', 'Sony')]\n",
    "\n",
    "############################### Test When All Others & Only Filer ###############################\n",
    "\n",
    "datapoint = test_point()\n",
    "datapoint['relations'] = [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'other', 'Sony Inc'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'other', 'Microsoft inc'],\n",
    "              ]\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(datapoint,\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= True,\n",
    "                            max_others= 0) \n",
    "assert llms_relations == [('ADVANCED MICRO DEVICES Inc', 'other', 'Sony'),\n",
    " ('ADVANCED MICRO DEVICES Inc', 'other', 'Microsoft')]\n",
    "assert other_relations == []\n",
    "\n",
    "############################### Test Adding LLM Relation Not Exist On OrgGroups With Only Filer ###############################\n",
    "datapoint = test_point()\n",
    "datapoint['relations'] = [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony Inc'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft inc'],\n",
    "  ['MISTAKE', 'supplier', 'WRONG NAME'],\n",
    "    \n",
    "              ]\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(datapoint,\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= False,\n",
    "                            max_others= 1) \n",
    "assert llms_relations == [('ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'),\n",
    " ('ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft')]\n",
    "\n",
    "assert other_relations == [('Microsoft', 'other', 'Sony')]\n",
    "\n",
    "\n",
    "#########Test Adding LLM Relation Not Exist On OrgGroups Without Only Filer & max_other=2  ###############################\n",
    "\n",
    "datapoint = test_point()\n",
    "datapoint['sentence'] = 'MISTAKE is supplier WRONG NAME of In addition, five customers, including Sony and Microsoft, accounted for approximately 95% of the net revenue attributable to ADVANCED MICRO DEVICES Inc Enterprise, Embedded and Semi Custom segment'\n",
    "datapoint['relations'] = [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony Inc'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft inc'],\n",
    "  ['MISTAKE', 'supplier', 'WRONG NAME'],\n",
    "              ]\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(datapoint,\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= False,\n",
    "                            max_others= 2) \n",
    "assert llms_relations == [('ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'),\n",
    " ('ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft'),\n",
    " ('MISTAKE', 'supplier', 'WRONG NAME')]\n",
    "assert len(other_relations) == 2\n",
    "\n",
    "\n",
    "#########Test having no relation  ###############################\n",
    "\n",
    "datapoint = test_point()\n",
    "datapoint['sentence'] = 'MISTAKE is supplier WRONG NAME of In addition, five customers, including Sony and Microsoft, accounted for approximately 95% of the net revenue attributable to ADVANCED MICRO DEVICES Inc Enterprise, Embedded and Semi Custom segment'\n",
    "datapoint['relations'] = []\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(datapoint,\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= False,\n",
    "                            max_others= 2) \n",
    "assert llms_relations == []\n",
    "assert len(other_relations) == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d720d715-bbb6-49a4-b0a7-07755542f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_point():\n",
    "    return {\n",
    " 'filer': 'ADVANCED MICRO DEVICES INC corp',\n",
    " 'sentence': 'In addition, five customers, including Sony and Microsoft, accounted for approximately 95% of the net revenue attributable to ADVANCED MICRO DEVICES Inc Enterprise, Embedded and Semi Custom segment',\n",
    " 'relations': [\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Sony'],\n",
    "  ['ADVANCED MICRO DEVICES Inc', 'supplier', 'Microsoft'],\n",
    "              ],\n",
    " 'org_groups': {'ADVANCED MICRO DEVICES Inc': 0, 'Microsoft': 1, 'Sony': 2}}\n",
    "\n",
    "\n",
    "############################### Test only_filer=true ###############################\n",
    "\n",
    "llms_relations, other_relations = data_aggregator.extract_relations_from_llm(test_point(),\n",
    "                            threshold= 0.9 ,\n",
    "                            only_filer= True,\n",
    "                            max_others= 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b7c1f-a5f9-4ab7-85dd-813f6a9793e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaBase",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
