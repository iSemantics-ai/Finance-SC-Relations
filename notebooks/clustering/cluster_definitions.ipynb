{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1a1083",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05f8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import math\n",
    "from typing import Dict, Any, Text, Tuple\n",
    "import yaml\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sklearn.cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33a5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61d9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac31bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine = \"local\"\n",
    "machine = \"paperspace\"\n",
    "\n",
    "if machine == \"local\":\n",
    "    src_dir= Path.cwd().parent    \n",
    "elif machine == \"paperspace\":\n",
    "    src_dir = Path(\"/notebooks/inferess-relation-extraction/\")\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "56654eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62be0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2023 12:37:47 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: hkunlp/instructor-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ad631184864d9ea3cab401a39de0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)06e59443b07dc819fb15c7233/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d3fce2243e465aace747b856412614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)3b07dc819fb15c7233/1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad0662f47fd42039ba2f90b0b4a6e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)443b07dc819fb15c7233/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdb447ddd204bed90a0faf21c291f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ff8ce4a26640f98a323d53e6ef16a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)84de506e59443b07dc819fb15c7233/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9ff574abcf4acc8023c80581bb594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)de506e59443b07dc819fb15c7233/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bef8794ac6f4bd4a1549035a2249087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)5c7233/config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f819032144c21b85874c65bafe6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba52ae0bfa74066865f8c979564da5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)dc819fb15c7233/sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d147133cdf5a4f9397b324cd6df683cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)07dc819fb15c7233/special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf54142d90f4eb4a75659e2b02f5f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a87725e387046bc80336caede420e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)06e59443b07dc819fb15c7233/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cabf86d368451bbb195da6c4f8f6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)3b07dc819fb15c7233/tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee0689530a7472da7f94eaf28c93871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)e506e59443b07dc819fb15c7233/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2023 12:38:19 - INFO - sentence_transformers.SentenceTransformer -   Use pytorch device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.matcher.core import SimCSE_Matcher\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "instructor_model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "\n",
    "# sent_embd_model = SimCSE_Matcher(\n",
    "#         model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#     )\n",
    "\n",
    "#sentence_emb_model = \"minilm\"\n",
    "sentence_emb_model = \"instructor\"\n",
    "\n",
    "def encode_sentences(sentence_list):\n",
    "    if sentence_emb_model == \"minilm\":\n",
    "        embeddings =  sent_embd_model.encode(sentence_list)\n",
    "        return embeddings.numpy()\n",
    "    elif sentence_emb_model == \"instructor\":\n",
    "        embeddings = instructor_model.encode(sentence_list)\n",
    "        return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d2236",
   "metadata": {},
   "source": [
    "### Test set from annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "339ee9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['definition', 'sentence', 'sme_relations', 'source_file', 'source_idx',\n",
      "       'join_index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "definitions_data = pd.read_excel(\"test_pipeline_data/cluster_definitions/all_0_1_definitions_clustered.xlsx\")\n",
    "\n",
    "#definitions_data = definitions_data.rename(columns={\"Unnamed: 0\": \"index\"})\n",
    "\n",
    "definitions_data = definitions_data[['definition', 'sentence', 'sme_relations', 'source_file', 'source_idx']]\n",
    "\n",
    "definitions_data[\"join_index\"] = definitions_data[\"source_idx\"]\n",
    "print(definitions_data.columns)\n",
    "\n",
    "pos_definitions_data = definitions_data[definitions_data[\"source_file\"] == \"/notebooks/inferess-relation-extraction/data/raw/llm_relations_all_label_1_v2_3.xlsx\"]\n",
    "neg_definitions_data = definitions_data[definitions_data[\"source_file\"] == \"/notebooks/inferess-relation-extraction/data/raw/llm_relations_other_from_label_0_v2_3.xlsx\"]\n",
    "\n",
    "\n",
    "\n",
    "pos_definitions_data.reset_index(inplace=True)\n",
    "neg_definitions_data.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "pos_org_data = pd.read_excel(\"test_pipeline_data/pipeline_train_data/llm_relations_all_label_1_v2_3.xlsx\")\n",
    "neg_org_data = pd.read_excel(\"test_pipeline_data/pipeline_train_data/llm_relations_other_from_label_0_v2_3.xlsx\")\n",
    "\n",
    "pos_org_data[\"join_index\"] = pos_org_data[\"index\"]\n",
    "neg_org_data[\"join_index\"] = neg_org_data[\"index\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bdc2b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # join the definitions with the original data, and only use two columns from original data\n",
    "pos_definitions_data = pos_definitions_data.merge(pos_org_data[[\"join_index\", \"concepts\", \"concept_class\"]], on=\"join_index\", how=\"left\")\n",
    "\n",
    "neg_definitions_data = neg_definitions_data.merge(neg_org_data[[\"join_index\",  \"concepts\", \"concept_class\"]], on=\"join_index\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f34206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b95a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'definition', 'sentence', 'sme_relations', 'source_file',\n",
       "       'source_idx', 'join_index', 'concepts', 'concept_class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_definitions_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "33201733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deduplicate_sentences(data, threshold=0.99):\n",
    "\n",
    "    # Compute embeddings for each sentence\n",
    "    sentence_list = data['definition'].to_list()\n",
    "    embeddings = encode_sentences(sentence_list)\n",
    "\n",
    "    groups = []\n",
    "    seen = set()\n",
    "\n",
    "    # Calculate the dot products on GPU\n",
    "    dot_products_cpu = np.dot(embeddings, embeddings.T)\n",
    "\n",
    "    # Create a mask for similarities > threshold\n",
    "    mask = dot_products_cpu > threshold\n",
    "\n",
    "    # Create groups based on mask\n",
    "    groups = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in range(mask.shape[0]):\n",
    "        if i not in seen:\n",
    "            # Find indices where similarity > threshold\n",
    "            similar_indices = np.where(mask[i])[0].tolist()\n",
    "\n",
    "            # Mark these as seen\n",
    "            seen.update(similar_indices)\n",
    "\n",
    "            # Append the group\n",
    "            groups.append(similar_indices)\n",
    "\n",
    "    # Keep one index from each group\n",
    "    indices_to_keep = [group[0] for group in groups]\n",
    "    sentences_to_ignored = [[sentence_list[idx]                                          \n",
    "                                         if type(sentence_list[idx]) == str \n",
    "                                         else sentence_list[idx][1]\n",
    "                                         for idx in group[1:]]\n",
    "                            for group in groups ]\n",
    "\n",
    "    return indices_to_keep, sentences_to_ignored\n",
    "\n",
    "\n",
    "def remove_duplicates_at_definitions_level(sentences_df):\n",
    "\n",
    "    sentences_df.loc[:, \"deduped\"] = False\n",
    "    sentences_df.loc[:, \"dup_definitions\"] = '[]'\n",
    "    sentences_df.loc[:, \"simillar_definitions_count\"] = 0\n",
    "    \n",
    "    \n",
    "    indices_to_keep, sentences_to_ignored = get_deduplicate_sentences(sentences_df, threshold=0.95)    \n",
    "    sentences_df.loc[indices_to_keep, \"deduped\"] = True\n",
    "    sentences_df.loc[indices_to_keep, \"dup_definitions\"] = [\"\\n\\n\".join(sents) for sents in sentences_to_ignored]\n",
    "    sentences_df.loc[indices_to_keep, \"simillar_definitions_count\"] = [len(sents) for sents in sentences_to_ignored]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d03eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2474d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb139928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_definitions(sentence_list):    \n",
    "    embeddings = encode_sentences(sentence_list)\n",
    "    clustering_model = sklearn.cluster.KMeans(n_clusters=50)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_labels = clustering_model.labels_\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "# Create an Excel writer object\n",
    "\n",
    "def write_clusers_to_spreadsheet(data, cluster_labels, out_file):\n",
    "    writer = pd.ExcelWriter(out_file)\n",
    "    data.loc[:, \"cluster_labels\"] = cluster_labels\n",
    "    data.to_excel(writer, sheet_name=\"all_clusters\", index=False)\n",
    "\n",
    "    # Get unique cluster labels (excluding -1, which represents noise)\n",
    "    unique_clusters = np.unique(cluster_labels[cluster_labels != -1])\n",
    "    for cluster_label in unique_clusters:\n",
    "        # Filter data for the current cluster label\n",
    "        cluster_data = data[data[\"cluster_labels\"] == cluster_label]\n",
    "\n",
    "        # Write the cluster data to a separate sheet\n",
    "        sheet_name = f\"{cluster_label}\"\n",
    "        cluster_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save the Excel file\n",
    "    writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7ef43c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167/1447189679.py:27: FutureWarning: save is not part of the public API, usage can give in unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "# # call\n",
    "# remove_duplicates_at_definitions_level(pos_definitions_data)\n",
    "# pos_definitions_data.to_excel(\"/notebooks/data_to_download/cluster_definition/pos_definitions_deduped.xlsx\", index =False)\n",
    "\n",
    "# pos_definitions_data = pos_definitions_data[pos_definitions_data.deduped]\n",
    "\n",
    "# sentence_list = pos_definitions_data['definition'].to_list()\n",
    "\n",
    "# cluster_labels = create_cluster_definitions(sentence_list)\n",
    "# out_file = \"/notebooks/data_to_download/cluster_definition/pos_clusters.xlsx\"\n",
    "# write_clusers_to_spreadsheet(pos_definitions_data, cluster_labels, out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0059f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178/1447189679.py:27: FutureWarning: save is not part of the public API, usage can give in unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "# call\n",
    "remove_duplicates_at_definitions_level(pos_definitions_data)\n",
    "pos_definitions_data.to_excel(\"/notebooks/data_to_download/cluster_definition/pos_definitions_deduped_instruct.xlsx\", index =False)\n",
    "\n",
    "pos_definitions_data = pos_definitions_data[pos_definitions_data.deduped]\n",
    "\n",
    "sentence_list = pos_definitions_data['definition'].to_list()\n",
    "sentence_list = [[\"Represent the finance sentence for clustering: \", sent] for sent in sentence_list]\n",
    "\n",
    "cluster_labels = create_cluster_definitions(sentence_list)\n",
    "out_file = \"/notebooks/data_to_download/cluster_definition/pos_clusters_instruct.xlsx\"\n",
    "pos_definitions_data = pos_definitions_data[[ 'definition', 'sentence', 'sme_relations', 'source_idx',\n",
    "                                               'concepts', 'concept_class', 'dup_definitions', 'simillar_definitions_count'\n",
    "                                               ]]\n",
    "write_clusers_to_spreadsheet(pos_definitions_data, cluster_labels, out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7ae4b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167/1447189679.py:27: FutureWarning: save is not part of the public API, usage can give in unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "# remove_duplicates_at_definitions_level(neg_definitions_data)\n",
    "# neg_definitions_data.to_excel(\"/notebooks/data_to_download/cluster_definition/neg_definitions_deduped.xlsx\", index =False)\n",
    "\n",
    "# neg_definitions_data = neg_definitions_data[neg_definitions_data.deduped]\n",
    "\n",
    "# sentence_list = neg_definitions_data['definition'].to_list()\n",
    "\n",
    "# cluster_labels = create_cluster_definitions(sentence_list)\n",
    "# out_file = \"/notebooks/data_to_download/cluster_definition/neg_clusters.xlsx\"\n",
    "# write_clusers_to_spreadsheet(neg_definitions_data, cluster_labels, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccf3ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178/1447189679.py:27: FutureWarning: save is not part of the public API, usage can give in unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "# call\n",
    "\n",
    "remove_duplicates_at_definitions_level(neg_definitions_data)\n",
    "neg_definitions_data.to_excel(\"/notebooks/data_to_download/cluster_definition/neg_definitions_deduped_instruct.xlsx\", index =False)\n",
    "\n",
    "neg_definitions_data = neg_definitions_data[neg_definitions_data.deduped]\n",
    "\n",
    "sentence_list = neg_definitions_data['definition'].to_list()\n",
    "sentence_list = [[\"Represent the finance sentence for clustering: \", sent] for sent in sentence_list]\n",
    "\n",
    "cluster_labels = create_cluster_definitions(sentence_list)\n",
    "out_file = \"/notebooks/data_to_download/cluster_definition/neg_clusters_instruct.xlsx\"\n",
    "neg_definitions_data = neg_definitions_data[[ 'definition', 'sentence', 'sme_relations', 'source_idx',\n",
    "                                               'concepts', 'concept_class', 'dup_definitions', 'simillar_definitions_count'\n",
    "                                               ]]\n",
    "\n",
    "write_clusers_to_spreadsheet(neg_definitions_data, cluster_labels, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cbe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
