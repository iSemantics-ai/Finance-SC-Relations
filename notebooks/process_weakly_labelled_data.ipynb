{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://admin:****@pypi.p.helpshift.com/simple\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas openai torch scikit-learn dvc dvc-s3\n",
    "#!pip install openpyxl retry python-dotenv\n",
    "\n",
    "#pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "#!pip install  tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import collections, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# from src.labels_generator import (generate_relations,relation_search,  resort_relation, relations_tupled, get_completion, deserialize_relations)\n",
    "\n",
    "# # Load matcher\n",
    "# from src.matcher.core import SimCSE_Matcher\n",
    "# matcher = SimCSE_Matcher(str(src_dir/ 'artifacts/matcher_model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine = \"local\"\n",
    "\n",
    "machine = \"paperspace\"\n",
    "\n",
    "if machine == \"local\":\n",
    "    src_dir= Path.cwd().parent    \n",
    "elif machine == \"paperspace\":\n",
    "    src_dir = Path(\"/notebooks/inferess-relation-extraction/\")\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install  tensorflow tensorflow-hub\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import numpy as np\n",
    "\n",
    "# use_model = hub.load(\"../../universal-sentence-encoder-large_5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.matcher.core import SimCSE_Matcher\n",
    "\n",
    "#from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "#instructor_model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "\n",
    "sent_embd_model = SimCSE_Matcher(\n",
    "        model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "sentence_emb_model = \"minilm\"\n",
    "\n",
    "def encode_sentences(sentence_list):\n",
    "    if sentence_emb_model == \"minilm\":\n",
    "        embeddings =  sent_embd_model.encode(sentence_list)\n",
    "        return embeddings.numpy()\n",
    "    elif sentence_emb_model == \"instructor\":\n",
    "        embeddings = instructor_model.encode(sentence_list)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train data\n",
    "\n",
    "#create sme_relation\n",
    "def generate_sme_relation(entity_1, entity_2, relationship ):\n",
    "    if relationship == \"customer\":\n",
    "        return [entity_1, \"supplier\" , entity_2]\n",
    "    elif relationship == \"supplier\":\n",
    "        return [entity_2, \"supplier\" , entity_1]\n",
    "    else:\n",
    "        return [entity_2, relationship , entity_1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deduplicate_sentences(data, text_column=\"sentence\",  threshold=0.99):\n",
    "\n",
    "    # Compute embeddings for each sentence\n",
    "    sentence_list = data[text_column].to_list()\n",
    "    embeddings = encode_sentences(sentence_list)\n",
    "\n",
    "    groups = []\n",
    "    seen = set()\n",
    "\n",
    "    # Calculate the dot products on GPU\n",
    "    dot_products_cpu = np.dot(embeddings, embeddings.T)\n",
    "\n",
    "    # Create a mask for similarities > threshold\n",
    "    mask = dot_products_cpu > threshold\n",
    "\n",
    "    # Create groups based on mask\n",
    "    groups = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in range(mask.shape[0]):\n",
    "        if i not in seen:\n",
    "            # Find indices where similarity > threshold\n",
    "            similar_indices = np.where(mask[i])[0].tolist()\n",
    "\n",
    "            # Mark these as seen\n",
    "            seen.update(similar_indices)\n",
    "\n",
    "            # Append the group\n",
    "            groups.append(similar_indices)\n",
    "\n",
    "    # Keep one index from each group\n",
    "    indices_to_keep = [group[0] for group in groups]\n",
    "    sentences_to_ignored = [[sentence_list[idx]                                          \n",
    "                                         if type(sentence_list[idx]) == str \n",
    "                                         else sentence_list[idx][1]\n",
    "                                         for idx in group[1:]]\n",
    "                            for group in groups ]\n",
    "\n",
    "    return indices_to_keep, sentences_to_ignored\n",
    "\n",
    "\n",
    "def remove_duplicates_at_filer_company_level(sentences_df):\n",
    "\n",
    "    sentences_df.loc[:, \"deduped\"] = False\n",
    "    sentences_df.loc[:, \"duplicate_sentences\"] = '[]'\n",
    "    \n",
    "    # group by accessionNumber and iterate over each group\n",
    "    for accessionNumber, group_df in tqdm(sentences_df.groupby(\"Acc no\")):\n",
    "\n",
    "        group_index_mapping = dict(zip(range(group_df.index.shape[0]), group_df.index))\n",
    "        indices_to_keep, sentences_to_ignored = get_deduplicate_sentences(group_df, threshold=0.95)\n",
    "        \n",
    "        indices_to_keep = [group_index_mapping[i] for i in indices_to_keep]\n",
    "        \n",
    "        sentences_df.loc[indices_to_keep, \"deduped\"] = True\n",
    "        sentences_df.loc[indices_to_keep, \"duplicate_sentences\"] = [\"\\n\\n\".join(sents) for sents in sentences_to_ignored]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'accessionNumber', 'filer', 'firstEntity', 'relationship',\n",
       "       'secondEntity', 'sentence', 'sme_relations', 'duplicate_sentences',\n",
       "       'sent_size', 'clause_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_neg_train_dedup_80.xlsx\")\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9029/9029 [02:02<00:00, 73.53it/s] \n"
     ]
    }
   ],
   "source": [
    "remove_duplicates_at_filer_company_level(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Acc no', 'filer', 'secondEntity', 'sentence', 'Prediction', 'Score',\n",
       "       'relationship', 'index', 'sme_relations', 'sent_size', 'clause_size',\n",
       "       'deduped', 'duplicate_sentences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     38962\n",
       "False     9337\n",
       "Name: deduped, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.deduped.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_neg_train_dedup_80_minilm_95.xlsx\", index = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove exact and cosine matching duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_excel(\"./../huge_labelled_data/train-customer-supplier-gen-07-set-02-rev-02-sentences-positive.xlsx\")\n",
    "# data = pd.read_csv(\"../huge_labelled_data/negative-training-sentences-2012-2020-bert-large.csv\")\n",
    "\n",
    "\n",
    "# data = data.drop_duplicates(subset='Sentence')\n",
    "\n",
    "# # For negative train data, Add and normalize the columns\n",
    "\n",
    "# data[\"relationship\"] = \"other\"\n",
    "# data = data.rename(columns={'Filer': 'filer', 'Company': 'secondEntity', 'Sentence': 'sentence'})\n",
    "# data[\"index\"] = data.index\n",
    "\n",
    "\n",
    "# # correct_sme_relation\n",
    "# tqdm.pandas(desc=\"Search relations\")\n",
    "# data['sme_relations'] =\\\n",
    "# data[['filer', 'secondEntity', 'relationship']]\\\n",
    "# .progress_apply(lambda x:\n",
    "# generate_sme_relation(\n",
    "# entity_1= x[0],\n",
    "# entity_2=x[1],\n",
    "# relationship=x[2]),axis=1).to_list()\n",
    "\n",
    "\n",
    "# # Remove duplicates in batches\n",
    "# batch_size = 10000\n",
    "# dedup_data_dfs = []\n",
    "# for batch_start in tqdm(range(0, data.shape[0], batch_size)):\n",
    "#     batch_data = data[batch_start: batch_start+batch_size]\n",
    "#     df_unique, _ = remove_duplicate_sentences_and_gen_new_dedup_df(batch_data)\n",
    "\n",
    "#     dedup_data_dfs.append(df_unique)\n",
    "\n",
    "\n",
    "# # Write data to disk\n",
    "# df_unique = pd.concat(dedup_data_dfs)\n",
    "\n",
    "# # Show unique DataFrame\n",
    "# print(df_unique.shape)\n",
    "\n",
    "# file_path = f'../huge_labelled_data/huge_neg_train_dedup_80.csv'\n",
    "\n",
    "# # Save the DataFrame to Excel\n",
    "# df_unique.to_excel(file_path, index=False)  # Set index=False if you don't want to save the index as a separate column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48299, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_neg_train_dedup_80_minilm_95.xlsx\")\n",
    "\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch GPU Exists..\n",
      "2023-10-22 05:49:58,787 â€” ðŸŒŒ spaCy â€” INFO â€” Language model used is en_core_web_trf\n",
      "2023-10-22 05:49:58,801 â€” ðŸŒŒ spaCy â€” INFO â€” spaCy Work On GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8663c5ba99c649dfbcf7fb42d305101c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)L6-v2/resolve/main/tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ed593280354a439b1c6a87b23bb7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)/all-MiniLM-L6-v2/resolve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18366ccec89e4fd9a0a9c5a1151bd268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)MiniLM-L6-v2/resolve/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ef12f141b846f3b0576b3f14b1095c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-v2/resolve/main/special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd09f3bdd4014c28ae4450a3659d090c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ll-MiniLM-L6-v2/resolve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0f72ca9ffa4b5e826f9fa611e20fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 05:50:17,858 â€” ðŸ’« Relations Extractor â€” INFO â€” Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 30005. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load with CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/22/2023 05:50:27 AM [INFO]: Loaded model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 05:50:27,251 â€” ðŸ’« Relations Extractor â€” INFO â€” Done!\n"
     ]
    }
   ],
   "source": [
    "from src.relation_extraction.infer import infer_from_trained\n",
    "from src.matcher.core import SimCSE_Matcher\n",
    "\n",
    "\n",
    "icon = \"\\U0001F4AB \"\n",
    "\n",
    "# Reference it in the inference container at /opt/ml/model/code\n",
    "def model_fn(model_dir: str):\n",
    "    \"\"\"\n",
    "    Loads the trained relation extractor and entity matcher models and returns them\n",
    "    as a tuple.\n",
    "    \"\"\"\n",
    "    relation_extractor = infer_from_trained(detect_entities=True,\n",
    "                             language_model=\"en_core_web_trf\",\n",
    "                             require_gpu=True,\n",
    "                            load_matcher=True,\n",
    "                             entity_matcher=str(src_dir / \"artifacts/matcher_model\"))\n",
    "    # \"pipeline-artifacts/matcher/all-MiniLM-Nli-All-Random-v4\"\n",
    "    entity_matcher = SimCSE_Matcher(\n",
    "        model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    model_args = {\"model_path\": os.path.join(model_dir, \"re_model\"), \"batch_size\": 32}\n",
    "    relation_extractor.load_model(model_args)\n",
    "    return relation_extractor, entity_matcher\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    relation_extractor, entity_matcher = model_fn(src_dir/ \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 06:12:06,765 â€” ðŸŒŒ spaCy â€” INFO â€” Start batch job for 3 chunks\n",
      "2023-10-22 06:12:06,767 â€” ðŸŒŒ spaCy â€” INFO â€” process chunk#1 ...\n",
      "2023-10-22 06:19:35,504 â€” ðŸŒŒ spaCy â€” INFO â€” process chunk#2 ...\n",
      "2023-10-22 06:25:45,351 â€” ðŸŒŒ spaCy â€” INFO â€” process chunk#3 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcb059fa2994c72a0386f41cf19896e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents, spans, group_docs, aliases_docs = relation_extractor.spacy_loader.predictor(data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45042, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert JSON strings to Python objects\n",
    "data.loc[:, \"sentence\"] = sents\n",
    "data.loc[:, \"spans\"] = spans\n",
    "data.loc[:, \"org_groups\"] = group_docs\n",
    "data.loc[:, \"aliases\"] = aliases_docs\n",
    "data.loc[:, 'num_orgs'] = data['org_groups']\\\n",
    "          .apply(lambda x : len(set(x.values()))).tolist()\n",
    "\n",
    "data = data[data['num_orgs'] > 1]\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter orgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only orgs having 'company_name', 'related_entity' companies \n",
    "\n",
    "def get_filtered_orgs(row):\n",
    "    \n",
    "    id2orgs = collections.defaultdict(list)\n",
    "    filtered_org_groups = {}\n",
    "    for k,v in row[\"org_groups\"].items():\n",
    "        id2orgs[v].append(k)\n",
    "\n",
    "    ent_comp = row[\"secondEntity\"].lower().split(\" \")[0].strip(\".,-:;\")\n",
    "    filer_comp = row[\"filer\"].lower().split(\" \")[0].strip(\".,-:;\")\n",
    "    filtered_org_id = {'filer': -1, 'ent_comp': -1}\n",
    "    for id, orgs in id2orgs.items():\n",
    "        \n",
    "        is_ent_comp_group =  any([True for x in orgs if re.search(r\"\\b{}\\b\".format(ent_comp), x.lower()) ])\n",
    "        is_filer_comp_group =  any([True for x in orgs if re.search(r\"\\b{}\\b\".format(filer_comp), x.lower())])\n",
    "        \n",
    "        if is_ent_comp_group:\n",
    "            filtered_org_id['ent_comp'] = id\n",
    "        elif is_filer_comp_group:\n",
    "            filtered_org_id['filer'] = id            \n",
    "\n",
    "        if filtered_org_id[\"filer\"] != -1 and filtered_org_id[\"ent_comp\"] != -1:\n",
    "            break\n",
    "    \n",
    "    if filtered_org_id[\"filer\"] != -1 and filtered_org_id[\"ent_comp\"] != -1:\n",
    "        for org in id2orgs[filtered_org_id[\"filer\"]]:\n",
    "            filtered_org_groups[org] = 0\n",
    "    \n",
    "        for org in id2orgs[filtered_org_id[\"ent_comp\"]]:\n",
    "            filtered_org_groups[org] = 1\n",
    "\n",
    "\n",
    "    return filtered_org_groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Acc no', 'filer', 'secondEntity', 'sentence', 'Prediction', 'Score',\n",
       "       'relationship', 'index', 'sme_relations', 'sent_size', 'clause_size',\n",
       "       'deduped', 'duplicate_sentences', 'spans', 'org_groups', 'aliases',\n",
       "       'num_orgs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.num_orgs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45042, 17)\n",
      "(41962, 19)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "\n",
    "# create filtered_org_groups column\n",
    "data.loc[:, 'filtered_org_groups'] = data.apply(get_filtered_orgs, axis=1)\n",
    "\n",
    "# keep only those rows where filtered_org_groups is not empty\n",
    "data.loc[:, 'num_orgs_filter'] = data['filtered_org_groups']\\\n",
    "          .apply(lambda x : len(set(x.values()))).tolist()\n",
    "\n",
    "data = data[data['num_orgs_filter'] > 1]\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clauses using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_clauses_count(doc):\n",
    "    clause_count = 0\n",
    "    for token in doc:\n",
    "        # if we find a verb, we try to find its subject(s) and object(s)\n",
    "        if token.pos_ == 'VERB':\n",
    "            clause_count+= 1\n",
    "\n",
    "    return clause_count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48299, 11)\n"
     ]
    }
   ],
   "source": [
    "# Filter large sentence using words count and number of clauses\n",
    "\n",
    "file_path = f'../huge_labelled_data/huge_neg_train_dedup_80.xlsx'\n",
    "\n",
    "df_unique = pd.read_excel(file_path)\n",
    "\n",
    "df_unique[\"sent_size\"] = df_unique[\"sentence\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "docs = [nlp(sent) for sent in df_unique[\"sentence\"].to_list()]\n",
    "df_unique[\"clause_size\"] = [get_clauses_count(doc) for doc in docs ]\n",
    "\n",
    "\n",
    "df_unique.to_excel(file_path,  index=False)  # Set index=False if you don't want to save the index as a separate column\n",
    "\n",
    "# Show unique DataFrame\n",
    "print(df_unique.shape)\n",
    "\n",
    "file_path = f'../huge_labelled_data/huge_neg_train_dedup_80_lg_sent.xlsx'\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "lg_condition = (df_unique[\"sent_size\"] > 65) &   (df_unique[\"clause_size\"] > 3)\n",
    "df_unique[lg_condition].to_excel(file_path,  index=False)  # Set index=False if you don't want to save the index as a separate column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32130, 12)\n"
     ]
    }
   ],
   "source": [
    "# Filter large sentence using words count and number of clauses\n",
    "\n",
    "\n",
    "file_path = f'../huge_labelled_data/huge_train_dedup_80.xlsx'\n",
    "\n",
    "df_unique = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "df_unique[\"sent_size\"] = df_unique[\"sentence\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "docs = [nlp(sent) for sent in df_unique[\"sentence\"].to_list()]\n",
    "df_unique[\"clause_size\"] = [get_clauses_count(doc) for doc in docs ]\n",
    "\n",
    "\n",
    "# Show unique DataFrame\n",
    "print(df_unique.shape)\n",
    "\n",
    "file_path = f'../huge_labelled_data/huge_train_dedup_80_lg_sent.xlsx'\n",
    "\n",
    "lg_condition = (df_unique[\"sent_size\"] > 50) & (df_unique[\"clause_size\"] > 3)\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "df_unique[lg_condition].to_excel(file_path,  index=False)  # Set index=False if you don't want to save the index as a separate column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_two_columns(row):\n",
    "    return row[\"filer\"] + \"--\" + row[\"secondEntity\"]\n",
    "\n",
    "df_unique[\"entity_pair\"] = df_unique.apply(join_two_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32130, 13)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1889"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(#len(df_unique[\"entity_pair\"].unique())\n",
    "\n",
    "#len(df_unique[(df_unique[\"sent_size\"] > 50) & (df_unique[\"clause_size\"] > 3)][\"entity_pair\"].unique())\n",
    "\n",
    "#df_unique[(df_unique[\"sent_size\"] > 50) & (df_unique[\"clause_size\"] > 5)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unique[(df_unique[\"sent_size\"] > 50) ][\"clause_size\"].count()\n",
    "\n",
    "# df_unique[(df_unique[\"clause_size\"] > 5) ][\"clause_size\"].count()\n",
    "\n",
    "# df_unique[(df_unique[\"sent_size\"] > 50)  & (df_unique[\"clause_size\"] > 5) ][\"clause_size\"].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root==> /notebooks/inferess-relation-extraction\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root==> /notebooks/inferess-relation-extraction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a9237f94cc47a8a8dd6606f617238c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)b/sec-bert-base/resolve/main/config.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbeb2e9ed444b3889eab669b0850dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 06:43:36,486 â€” SCClassifier â€” INFO â€” loading checkpoint from `sc_model`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24af49bc459642a19e5d06f3958f98c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-base/resolve/main/tokenizer_config.json:   0%|          | 0.00/263 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e804d16f79e64bec8299f1109ba5b247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ueb/sec-bert-base/resolve/main/vocab.txt:   0%|          | 0.00/221k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcd5f26e4cf48f7819cf90f624b5667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ase/resolve/main/special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 06:43:37,608 â€” SCClassifier â€” INFO â€” inference mode...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import  src.sc_classifier.config.core as sscc\n",
    "importlib.reload(sscc)\n",
    "\n",
    "from src.sc_classifier.trainer import Trainer\n",
    "from src.sc_classifier.config.core import config\n",
    "\n",
    "\n",
    "config.train_args.load_pretrained = True\n",
    "\n",
    "sc_model = Trainer(config=config,load_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'accessionNumber', 'filer', 'firstEntity', 'relationship',\n",
       "       'secondEntity', 'sentence', 'sme_relations', 'duplicate_sentences',\n",
       "       'sent_size', 'clause_size', 'deduped'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_train_dedup_80_minilm_95.xlsx\")\n",
    "# data = data[data[\"deduped\"] == True]\n",
    "# data.reset_index(inplace=True)\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 933/933 [05:45<00:00,  2.70batch/s]\n"
     ]
    }
   ],
   "source": [
    "scores, preds = sc_model.predict_seq(data['sentence'] , max_length=128)\n",
    "data.loc[:, 'sc_score'] = scores.max(1)\n",
    "data.loc[:, 'sc_label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6908, 21)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data[\"sc_label\"] == 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 21)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data[\"sc_score\"] > 0.95) & (data[\"sc_label\"] == 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f24cc945eb4228a5d09a6f9448efda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)/sec-bert-shape/resolve/main/config.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52ba64b9c0b4b82af0ab765610c9f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 06:57:01,475 â€” SCClassifier â€” INFO â€” loading checkpoint from `concept_model`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b9e9c153340edbf26b0ad2925f56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)shape/resolve/main/tokenizer_config.json:   0%|          | 0.00/4.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3799a0c96f545009dc2c7fe55c0d23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)eb/sec-bert-shape/resolve/main/vocab.txt:   0%|          | 0.00/227k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a1fff618594744b0a931135fbc851c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ape/resolve/main/special_tokens_map.json:   0%|          | 0.00/3.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 06:57:01,982 â€” SCClassifier â€” INFO â€” inference mode...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys \n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from src.sc_classifier.config.core import config, PACKAGE_ROOT\n",
    "from src.sc_classifier.trainer import Trainer\n",
    "from src.sc_classifier.models import constructor\n",
    "from src.utils import dict2dot\n",
    "\n",
    "# Read params file\n",
    "with open(PACKAGE_ROOT / 'params.yaml') as o:\n",
    "    params = dict2dot(yaml.safe_load(o))\n",
    "    \n",
    "# mutate train args to fit\n",
    "config.app_config.package_name = params.concept_train.package_name\n",
    "config.ml_model_config.target = \"concept_class_remapped\"\n",
    "config.ml_model_config.classes = params['concept_train']['classes']\n",
    "config.train_args = params.concept_train\n",
    "config.train_args.load_pretrained = True \n",
    "\n",
    "cc_model = Trainer(\n",
    "        loss_function=CrossEntropyLoss() , \n",
    "        optimizer=AdamW,\n",
    "        load_data=False,\n",
    "        model_name= \"concept_model\",\n",
    "        config = config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 656/656 [16:56<00:00,  1.55s/batch]\n"
     ]
    }
   ],
   "source": [
    "scores, labels = cc_model.predict(data.sentence.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_scores  =  list(map(lambda x: {cc_model.model.config.id2label[k]:v for k,v in enumerate(x)} , scores))\n",
    "soreted_scores = list(map(lambda x: sorted(x.items(), key=lambda x: x[1], reverse=True) , classes_scores))\n",
    "predict_cc_classes = list(map(lambda x: cc_model.model.config.id2label[x], labels))\n",
    "predict_cc_classes_2 = list(map(lambda x: x[1][0], soreted_scores))\n",
    "\n",
    "predict_cc_class_score = list(map(lambda x: x[0][1], soreted_scores))\n",
    "predict_cc_class_2_score = list(map(lambda x: x[1][1], soreted_scores))\n",
    "\n",
    "\n",
    "top_2 = list(map(lambda x: [x[0][0], x[1][0]] , soreted_scores ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cc_class\"] = predict_cc_classes\n",
    "data[\"cc_class_2\"] = predict_cc_classes_2\n",
    "\n",
    "data[\"cc_class_score\"] = predict_cc_class_score\n",
    "data[\"cc_class_2_score\"] = predict_cc_class_2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "financial_statements    48.490900\n",
       "licensing_and_ip        22.054544\n",
       "agreement               21.580989\n",
       "supply_chain             4.789753\n",
       "revenue                  2.059679\n",
       "real_estate              1.024134\n",
       "Name: cc_class, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value_counts in percentage\n",
    "data[data[\"sc_label\"] == 0][\"cc_class\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agreement               27.012160\n",
       "revenue                 24.073538\n",
       "supply_chain            21.033584\n",
       "financial_statements    14.490446\n",
       "licensing_and_ip         9.192241\n",
       "real_estate              4.198031\n",
       "Name: cc_class, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"sc_label\"] == 1][\"cc_class\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agreement               30.632730\n",
       "supply_chain            21.401685\n",
       "revenue                 16.400789\n",
       "financial_statements    15.809285\n",
       "licensing_and_ip        10.772540\n",
       "real_estate              4.982972\n",
       "Name: cc_class, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data[\"sc_score\"] < 0.95) & (data[\"sc_label\"] == 1)][\"cc_class\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[sc_filter][\"cc_class_2\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_neg_train_dedup_sc_cc.xlsx\", index = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40990\n",
      "-rw-r--r--  1 root root 6735463 Oct 20 16:39 huge_neg_train_dedup_80.xlsx\n",
      "-rw-r--r--  1 root root 5626623 Oct 20 16:39 huge_train_dedup_80.xlsx\n",
      "-rw-r--r--  1 root root 4614462 Oct 20 17:34 huge_train_dedup_80_minilm_95.xlsx\n",
      "-rw-r--r--  1 root root 7398062 Oct 20 23:51 huge_neg_train_dedup_80_minilm_95.xlsx\n",
      "drwxr-xr-x 10 root root       8 Oct 21 01:03 ..\n",
      "-rw-r--r--  1 root root 7979382 Oct 21 05:03 huge_train_dedup_sc_cc.xlsx\n",
      "drwxr-xr-x  3 root root       7 Oct 21 18:50 .\n",
      "drwxr-xr-x  2 root root       4 Oct 21 19:27 clusters_pos\n",
      "-rw-r--r--  1 root root 9617762 Oct 22 06:42 huge_train_dedup_sc_cc_re.xlsx\n"
     ]
    }
   ],
   "source": [
    "!ls -lart test_pipeline_data/huge_set_deduped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_<>_train_dedup_sc_cc.xlsx\")\n",
    "\n",
    "data.loc[:, \"spans\"] = data[\"spans\"].apply(eval)\n",
    "data.loc[:, \"org_groups\"] = data[\"org_groups\"].apply(eval)\n",
    "data.loc[:, \"filtered_org_groups\"] = data.filtered_org_groups.apply(lambda x: eval(x))\n",
    "data.loc[:, \"aliases\"] = data[\"aliases\"].apply(eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mutate text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266566/266566 [01:12<00:00, 3663.95it/s]\n",
      "10/22/2023 07:20:38 AM [INFO]: Tokenizing data...\n",
      "tokenization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266566/266566 [02:09<00:00, 2059.93it/s]\n",
      "tags positioning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266566/266566 [00:05<00:00, 46993.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Invalid rows/total: 0/266566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8331/8331 [34:24<00:00,  4.03it/s] \n",
      "mutate text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266566/266566 [01:11<00:00, 3740.87it/s]\n",
      "10/22/2023 07:58:33 AM [INFO]: Tokenizing data...\n",
      "tokenization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266566/266566 [02:18<00:00, 1926.90it/s]\n",
      "tags positioning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266566/266566 [00:06<00:00, 42318.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Invalid rows/total: 0/266566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8331/8331 [35:24<00:00,  3.92it/s] \n",
      "/notebooks/inferess-relation-extraction/src/relation_extraction/infer.py:349: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict relations\n",
    "predictions= relation_extractor.predict_relations(\n",
    "    sentences=data[\"sentence\"].tolist(),\n",
    "    ent=\"ORG\",\n",
    "    spans=data[\"spans\"].tolist(),\n",
    "    org_groups=data[\"org_groups\"].tolist(),\n",
    "    aliases=data[\"aliases\"].tolist(),\n",
    "    mutate=True, # re_model trained to predict\n",
    "    reverse=True # aggregate average score between both directions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'accessionNumber', 'filer', 'firstEntity',\n",
       "       'relationship', 'secondEntity', 'sentence', 'sme_relations',\n",
       "       'duplicate_sentences', 'sent_size', 'clause_size', 'deduped',\n",
       "       'sc_score', 'sc_label', 'cc_class', 'cc_class_2', 'cc_class_score',\n",
       "       'cc_class_2_score', 'spans', 'org_groups', 'aliases', 'num_orgs',\n",
       "       'filtered_org_groups', 'num_orgs_filter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['re_relations'] = None\n",
    "data.loc[predictions.index.values, \"re_relations\"] = predictions[\"relations\"]\n",
    "data.dropna(subset=['re_relations'],inplace=True)\n",
    "data['re_score'] = data['re_relations'].apply(lambda x: x[0]['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41778, 27)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[[\"filer\", \"sme_relations\", \"relationship\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_re_relation_e2_to_e1(row):\n",
    "    reporter_name = row[\"filer\"]\n",
    "    reported_company = row[\"secondEntity\"]\n",
    "    reporter_first_word = reporter_name.split(\" \")[0].strip(\".,-:;\").lower()\n",
    "    reported_first_word = reported_company.split(\" \")[0].strip(\".,-:;\").lower()\n",
    "\n",
    "    relation = row[\"re_relations\"][0]\n",
    "\n",
    "    entity_companies = [key for key in relation.keys() if key != \"score\"]\n",
    "    company1 = entity_companies[0]\n",
    "    company2 = entity_companies[1]\n",
    "    company1_lower = company1.lower()\n",
    "    company2_lower = company2.lower()\n",
    "\n",
    "    reported_company_relation = None\n",
    "    \n",
    "    # TODO - find the correct relation\n",
    "    \n",
    "    if reporter_first_word == reported_first_word:\n",
    "        # if both words are same, then deal with this situation later\n",
    "        reported_company_relation = \"TBC\"\n",
    "    elif reporter_first_word in company1_lower or reported_first_word in company2_lower:                    \n",
    "        reported_company_relation = relation[company2]\n",
    "    elif reporter_first_word in company2_lower or reported_first_word in company1_lower:\n",
    "        reported_company_relation = relation[company1]\n",
    "    \n",
    "    return reported_company_relation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, \"re_rel_label\"] = data.apply(get_re_relation_e2_to_e1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer    4619\n",
       "other       1137\n",
       "supplier     945\n",
       "None         150\n",
       "Name: re_rel_label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"sc_label\"] == 1 ].re_rel_label.value_counts(dropna=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other       31239\n",
       "customer     7606\n",
       "supplier     1714\n",
       "None         1219\n",
       "Name: re_rel_label, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.re_rel_label.value_counts(dropna=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer    3588\n",
       "other       1024\n",
       "supplier     805\n",
       "None         132\n",
       "Name: re_rel_label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[((data[\"sc_label\"] == 1) & (data[\"sc_score\"] < 0.95))].re_rel_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6660, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_sc_sents = data[((data[\"sc_label\"] == 1) & (data[\"sc_score\"] < 0.95))]\n",
    "\n",
    "missed_sc_sents[missed_sc_sents[\"re_score\"] > 0.95].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer    6894\n",
       "supplier    2412\n",
       "other        593\n",
       "None         227\n",
       "Name: re_rel_label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_sc_sents.re_rel_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer    4751\n",
       "supplier    1316\n",
       "other        393\n",
       "None         200\n",
       "Name: re_rel_label, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_sc_sents[missed_sc_sents[\"re_score\"] > 0.95].re_rel_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer    16051\n",
       "supplier     3834\n",
       "other        2983\n",
       "None          722\n",
       "Name: re_rel_label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.re_rel_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if relationship column matches with re_rel_label column\n",
    "def get_re_pred_match(row):\n",
    "    if row[\"re_rel_label\"] == None:\n",
    "        return None\n",
    "    elif row[\"re_rel_label\"] in (\"TBC\", \"other\"):\n",
    "        return None\n",
    "    elif row[\"relationship\"] == row[\"re_rel_label\"]:\n",
    "        return True\n",
    "    elif row[\"relationship\"] != row[\"re_rel_label\"]:\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# check if relationship column matches with re_rel_label column\n",
    "def get_re_pred_other_match(row):\n",
    "    if row[\"re_rel_label\"] == None:\n",
    "        return None\n",
    "    elif row[\"re_rel_label\"] in (\"TBC\", \"other\"):\n",
    "        return True\n",
    "    elif row[\"relationship\"] == row[\"re_rel_label\"]:\n",
    "        return True\n",
    "    elif row[\"relationship\"] != row[\"re_rel_label\"]:\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos\n",
    "#data.loc[:, \"re_pred_match\"] = data.apply(get_re_pred_match, axis=1)\n",
    "\n",
    "# neg\n",
    "data.loc[:, \"re_pred_match\"] = data.apply(get_re_pred_other_match, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other    41778\n",
       "Name: relationship, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.relationship.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     31239\n",
       "False     9320\n",
       "None      1219\n",
       "Name: re_pred_match, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.re_pred_match.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"./test_pipeline_data/huge_set_deduped_data/huge_neg_train_dedup_sc_cc_re.xlsx\", index = False )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
